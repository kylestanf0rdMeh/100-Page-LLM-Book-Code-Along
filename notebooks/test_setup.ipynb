{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# building block for creating models (pytorch handles derivitave calculations)\n",
    "import torch.nn as nn\n",
    "# for algorithms like gradient descent, optim handles this\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sequential is well suited for simple feed-forward netowrks where data flows sequentailly through layers. Each layer's output naturally becomes the input for the subsequent layers. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# n_inputs handles the size of our feature vector z, while the output dimensionality n_outputs determines the layers unit count. \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mn_inputs\u001b[49m, n_outputs),\n\u001b[0;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSigmoid()    \n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Sequential is well suited for simple feed-forward netowrks where data flows sequentailly through layers. Each layer's output naturally becomes the input for the subsequent layers. \n",
    "model = nn.Sequential(\n",
    "    # n_inputs handles the size of our feature vector z, while the output dimensionality n_outputs determines the layers unit count. \n",
    "    nn.Linear(n_inputs, n_outputs),\n",
    "    # transofrms z through the sigmoid function to produce the output score\n",
    "    nn.Sigmoid()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.define dataset \n",
    "2. create model instance \n",
    "3. establish binary cross-entropy loss function \n",
    "4. set up our gradient descent algorithm.\n",
    "\"\"\"\n",
    "# 1. \n",
    "inputs = torch.tensor([\n",
    "    [22, 25], [25, 35], [47,80], [52, 95], [46, 82], [56,90],\n",
    "    [23, 27],[30,50],[40,60],[39,57],[53,95],[48,88]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([\n",
    "    [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\"\"\"\n",
    "NOTE: See how the 12 rows in the matriz match up with the 12 components in our vector? From our learnings yesterday\n",
    "\"\"\"\n",
    "\n",
    "# 2. \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inputs.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "ALTERNATIVE\n",
    "\n",
    "(basic 2 layer FNN)\n",
    "\n",
    "each of the 100 units in the first layer contains 2 weights and 1 bias, while the output layer's single unit has 100 weights and 1 bias. Automatic differentiation handles gradient computation internally, so the remaining code stays unchanged.\n",
    "\"\"\"\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(features.shape[1], 100),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(100, labels.shape[1]),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# uses gradient descent (list of model params, learning rates as inputs)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. \n",
    "criterion = nn.BCELoss() #binary cross entropy loss\n",
    "\n",
    "inputs.shape #torch.size([12, 2])\n",
    "\n",
    "# just remember tha tensors are pytorchs optimized core data structures for running on both cpu and gpu, (multi-dimensional arrays\n",
    "# remember that setting the tensor with dtype = torch.float32 essential for nn computations\n",
    "# however, other precision is available like 16 or 8 bit floats and integers which is used for quantization (helps reduce model size, improve computationla efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(500):\n",
    "    # calcs binary cross entropy (evaluates model predictions against training labels)\n",
    "    # clears gradients at each steps beginning\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model(inputs), labels)\n",
    "    # prompts pytorch to traverse fthis graph and compute gradients via the chain rule, eliminatiung need for manual gradient derivation and implementation\n",
    "    loss.backwards()\n",
    "    # updated by substracting the product of the learning rate and the loss's function partial deriviates.\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-book-env",
   "language": "python",
   "name": "llm-book-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
